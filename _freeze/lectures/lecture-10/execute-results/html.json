{
  "hash": "9a3420cd1baa35890362cae04c7b241c",
  "result": {
    "markdown": "---\ntitle: \"Lecture 10\"\nsubtitle: Bayes' Rule\nformat: html\ndate: 2023-10-03\n---\n\n\n\n\n## Section 4.5 Bayes' Rule\n\n- Remember the experiment on color blindness:\n\n|                        | Men($B$) | Women($B^c$) | Total |\n|------------------------|----------|--------------|-------|\n| Colorblind ($A$)       | .04      | .002         | .042  |\n| Not Colorblind ($A^c$) | .47      | .488         | .958  |\n| Total                  | .51      | .49          | 1     |\n: Rates of color blindeness in men and women\n\n- Event $B$: person is a man\n- Event $B^c$: person is a woman\\\n\nTaken together it's the full sample space and are mutually exclusive. \n\n- What if we want to describe all color blind people? We have two types, those in $B$ and those in $B^c$\n\n- How do we represent color blind people in $B$?\n    - $A \\cap B$\n\n- How do we present color blind people in $B^c$?\n    - $A \\cap B^c$\n\n- How do we represent all color blind people:\n\n- $(A \\cap B) \\cup (A \\cap B^c)$\\\n\nUsing the addition rule:\n\n$$\nP(A) = P(A \\cap B) + P(A \\cap B^c)\n$$\n\nWhy is:\n\n$$\nP((A \\cap B) \\cap (A \\cap B^c)) = 0\n$$\n\n\nLet's now assume there are more than 2 *mutually exclusive and exhaustive* categories - we can extend the rule using *k* populations $S_1, S_2, S_3, \\dots, S_k$.\n\n$$\nA = (A \\cap S_1) \\cup (A \\cap S_2) \\cup (A \\cap S_3) \\cup \\dots \\cup (A \\cap S_k)\n$$\n\nThen\n$$\nP(A) = P(A \\cap S_1) + P(A \\cap S_2) + P(A \\cap S_3) + \\dots + P(A \\cap S_k)\n$$\n\n\n- Show venn diagram that is on page 159\n\n\nRemember also that:\n\n$$\nP(A \\cap S_i) = P(S_i)P(A|S_i)\n$$\n\n\n\n### The Law of Total Probability\n\n$$\nP(A) = P(S_1)P(A|S_1) + P(S_2)P(A|S_2) + P(S_3)P(A|S_3) + \\dots + P(S_k)P(A|S_k) \n$$\n\n\n\n### Bayes' Rule\n\nOne useful use of this is when you need to find the conditional probability that A occured given B $P(A|B)$, but you know $P(B|A)$. This is common with screening tests. \n\n- Event $D$ is that you have the disease\n- Event $T$ is that the test is positive\n\nA test is often characterized by the true positive rate $P(T|D)$, true negative rate $P(T^c|D^c)$, false postive rate $P(T|D^c)$ and the false negative rate $P(T^c|D)$. You can see all these values depend on  a state of disease. This is because when they are creating a new test, they can find people with the disease and without the disease - i.e. they are known to have the disease.\n\n- However, what we really care about is the probability you have the disease given a positive test $P(D|T)$.\n\n\nLet's imagine a scenario where you have a test that is pretty good when it detects $P(T|D) = .99$ and doesn't have a ton of false positives $P(T|D^c) = .01$. Does this seem like a good test?\\ \n\\\n\\\n\\\nThe answer is that it depends. Let's see why:\n\nRecall that: \n\n$$\nP(D)(P(T|D)) = P(D\\cap T) = P(T)P(D|T)\n$$\n\nThen:\n\n$$\nP(D|T) = \\frac{P(T|D)P(D)}{P(T)}\n$$\n\nWhere \n\n$$\nP(T) = P(D)P(T|D) + P(D^c)P(T|D^c)\n$$\n\nPlugging in for $P(T)$:\n\n$$\nP(D|T) = \\frac{P(T|D)P(D)}{P(D)P(T|D) + P(D^c)P(T|D^c)}\n$$\n\n\nWhat can we fill in?\n\n$P(T|D) = .99$ and $P(T|D^c) = .01$\n\n$$\nP(D|T) = \\frac{.99P(D)}{.99P(D) + .01P(D^c)}\n$$\n\n\nLet's say we are trying to detect the common cold. What if it's the winter? There is a lot of cold out there, assuming you are feeling sick then there is a 40% chance it's the cold.\n\nTherefore: $P(D) = .4$ and $P(D^c) = 1 - .4 = .6$\nSo what's the probability that you have the cold given there is a positive test?\n\n$$\nP(D|T) = \\frac{.99 \\times .4}{.99 \\times .4 + .01 \\times .6} = 0.985 = 98.5\\%\n$$\n\n\nHowever, what if it's the summer - there isn't a lot of cold going around out there. Not a lot of people have the cold so $P(D) = .05$ and $P(D^c) = .95$.\n\n$$\nP(D|T) = \\frac{.99 \\times .05}{.99 \\times .05 + .01 \\times .95} = 0.839 = 83.9\\%\n$$\n\nThe posterior probability $P(D|T)$ changes even though the test informaton is the same. It changes because the prior ($P(D)$) matters.\n\\\n\\\n\\\n\\\n\\\n\\\nThis can be generalized to:\n\n\n\n$$\nP(S_i|A) = \\frac{P(S_i)P(A|S_i)}{\\sum_{j=1}^kP(S_j)P(A|S_j)}\n$$\n\nfor $i = 1, 2, \\dots, k$\n\n\n### Homework\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"4.5.1-3, 4.5.9, 4.5.15\"\n```\n:::\n:::\n\n\nAnswers: [Chapter 4 - Section 5](/homework/chapter-04.qmd#section-4.5)",
    "supporting": [
      "lecture-10_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}